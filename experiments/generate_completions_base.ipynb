{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# ! pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama huggingface_hub accelerate\n",
    "# ! pip install bitsandbytes==0.42.0 peft litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from jaxtyping import Float, Int\n",
    "from colorama import Fore\n",
    "from huggingface_hub import notebook_login\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List, Dict, Union, Callable\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftConfig, PeftModel, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(771)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9f0c275f2445ae957db9f5e037e0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-chat-hf into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8dec1728ea4290b0a1890a9acc976e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Ireland is Dublin. Unterscheidung between the two is not always clear'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLAMA_2_7B_CHAT_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "inference_dtype = torch.float16\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=inference_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_2_7B_CHAT_PATH,\n",
    "    torch_dtype=inference_dtype,\n",
    "    device_map = \"cuda:0\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "hooked_base_model = HookedTransformer.from_pretrained_no_processing(\n",
    "  LLAMA_2_7B_CHAT_PATH,\n",
    "  hf_model=base_model,\n",
    "  dtype=inference_dtype,\n",
    "  device=DEVICE,\n",
    "  default_padding_side='left',\n",
    ")\n",
    "\n",
    "del base_model\n",
    "\n",
    "hooked_base_model.tokenizer.padding_side = 'left'\n",
    "hooked_base_model.tokenizer.pad_token = \"[PAD]\"\n",
    "\n",
    "hooked_base_model.generate(\"The capital of Ireland is\", max_new_tokens=10, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"base\"\n",
    "model = hooked_base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup base dataset (harmful / harmless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_CHAT_TEMPLATE = \"\"\"\n",
    "[INST]{prompt}[/INST]\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_instructions_chat(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: List[str]\n",
    ") -> Int[Tensor, 'batch_size seq_len']:\n",
    "    prompts = [LLAMA_CHAT_TEMPLATE.format(prompt=instruction) for instruction in instructions]\n",
    "    # return tokenizer(prompts, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\").input_ids\n",
    "    return tokenizer(prompts, padding=True, truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "tokenize_instructions_fn = functools.partial(tokenize_instructions_chat, tokenizer=model.tokenizer)\n",
    "\n",
    "def get_harmful_instructions():\n",
    "    url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
    "    instructions = dataset['goal'].tolist()\n",
    "\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test\n",
    "\n",
    "def get_harmless_instructions():\n",
    "    hf_path = 'tatsu-lab/alpaca'\n",
    "    dataset = load_dataset(hf_path)\n",
    "\n",
    "    # filter for instructions that do not have inputs\n",
    "    instructions = []\n",
    "    for i in range(len(dataset['train'])):\n",
    "        if dataset['train'][i]['input'].strip() == '':\n",
    "            instructions.append(dataset['train'][i]['instruction'])\n",
    "\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_with_hooks(\n",
    "    model: HookedTransformer,\n",
    "    toks: Int[Tensor, 'batch_size seq_len'],\n",
    "    max_tokens_generated: int = 64,\n",
    "    fwd_hooks = [],\n",
    ") -> List[str]:\n",
    "\n",
    "    all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=toks.device)\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "\n",
    "    for i in range(max_tokens_generated):\n",
    "        with model.hooks(fwd_hooks=fwd_hooks):\n",
    "            logits = model(all_toks[:, :-max_tokens_generated + i])\n",
    "            next_tokens = logits[:, -1, :].argmax(dim=-1) # greedy sampling (temperature=0)\n",
    "            all_toks[:,-max_tokens_generated+i] = next_tokens\n",
    "\n",
    "    return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "def get_generations(\n",
    "    model: HookedTransformer,\n",
    "    instructions: List[str],\n",
    "    tokenize_instructions_fn: Callable[[List[str]], Int[Tensor, 'batch_size seq_len']],\n",
    "    fwd_hooks = [],\n",
    "    max_tokens_generated: int = 64,\n",
    "    batch_size: int = 16,\n",
    ") -> List[Dict[str, Union[str, bool]]]:\n",
    "\n",
    "    generations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size]).to(DEVICE)\n",
    "        generation = _generate_with_hooks(\n",
    "            model,\n",
    "            toks,\n",
    "            max_tokens_generated=max_tokens_generated,\n",
    "            fwd_hooks=fwd_hooks,\n",
    "        )\n",
    "\n",
    "        for instruction, completion in zip(instructions[i:i+batch_size], generation):\n",
    "            generations.append({\n",
    "              \"instruction\": instruction,\n",
    "              \"completion\": completion,\n",
    "            })\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return generations\n",
    "\n",
    "def direction_ablation_hook(\n",
    "    activation: Float[Tensor, \"... d_act\"],\n",
    "    hook: HookPoint,\n",
    "    direction: Float[Tensor, \"d_act\"]\n",
    "):\n",
    "    direction = direction.to(DEVICE)\n",
    "    proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction\n",
    "    return activation - proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train harmful: 416, harmless: 25058\n",
      "Test harmful: 104, harmless: 6265\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train harmful: {len(harmful_inst_train)}, harmless: {len(harmless_inst_train)}\")\n",
    "print(f\"Test harmful: {len(harmful_inst_test)}, harmless: {len(harmless_inst_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refsual vector sample size: 300\n",
      "Test Set size: 320\n"
     ]
    }
   ],
   "source": [
    "refusal_vector_sample_size = 300\n",
    "refusal_vector_harmful = harmful_inst_train[:refusal_vector_sample_size]\n",
    "refusal_vector_harmless = harmless_inst_train[:refusal_vector_sample_size]\n",
    "\n",
    "with open('../datasets/adversarial_questions_gpt4o.json') as fr:\n",
    "    adversarial_qs = json.load(fr)\n",
    "\n",
    "harmful_test = harmful_inst_train[refusal_vector_sample_size:] + harmful_inst_test + adversarial_qs\n",
    "print(f\"Refsual vector sample size: {len(refusal_vector_harmful)}\\nTest Set size: {len(harmful_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refusal_direction(refusal_vector_harmful, refusal_vector_harmless, n_samples, layer, batch_size=32):\n",
    "    harmful_acts, harmless_acts = [], []\n",
    "    harmful_samples = refusal_vector_harmful\n",
    "    harmless_samples = refusal_vector_harmless\n",
    "\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        # tokenize instructions\n",
    "        harmful_toks = tokenize_instructions_fn(instructions=harmful_samples[i:i+batch_size])\n",
    "        harmless_toks = tokenize_instructions_fn(instructions=harmless_samples[i:i+batch_size])\n",
    "\n",
    "        # run model on harmful and harmless instructions, caching intermediate activations\n",
    "        _, harmful_cache = model.run_with_cache(harmful_toks, names_filter=lambda hook_name: 'resid' in hook_name)\n",
    "        _, harmless_cache = model.run_with_cache(harmless_toks, names_filter=lambda hook_name: 'resid' in hook_name)\n",
    "\n",
    "        # compute difference of means between harmful and harmless activations at an intermediate layer\n",
    "        # use the last token position \n",
    "        harmful_acts.append(harmful_cache['resid_pre', layer][:, -1, :])\n",
    "        harmless_acts.append(harmless_cache['resid_pre', layer][:, -1, :])\n",
    "\n",
    "    harmful_acts = torch.cat(harmful_acts, dim=0)\n",
    "    harmless_acts = torch.cat(harmless_acts, dim=0)\n",
    "\n",
    "    # Remove top PCA direction\n",
    "    all_acts = np.vstack((harmful_acts.cpu().numpy(), harmless_acts.cpu().numpy()))\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    top_component = pca.fit_transform(all_acts)\n",
    "    all_acts_residual = all_acts - np.dot(top_component, pca.components_)\n",
    "    \n",
    "    harmful_acts = torch.tensor(all_acts_residual[:len(harmful_acts)], device=DEVICE, dtype=torch.float16)\n",
    "    harmless_acts = torch.tensor(all_acts_residual[len(harmful_acts):], device=DEVICE, dtype=torch.float16)\n",
    "\n",
    "    # Calculate refusal vector\n",
    "    harmful_mean_act = harmful_acts.mean(dim=0)\n",
    "    harmless_mean_act = harmless_acts.mean(dim=0)\n",
    "\n",
    "    refusal_dir = harmful_mean_act - harmless_mean_act\n",
    "    refusal_dir = refusal_dir / refusal_dir.norm()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return refusal_dir, harmful_acts, harmless_acts\n",
    "\n",
    "\n",
    "def get_intervention_completions(refusal_dir, eval_samples, test_set, batch_size=12):\n",
    "    intervention_dir = refusal_dir.to(DEVICE)\n",
    "    intervention_layers = list(range(model.cfg.n_layers)) # all layers\n",
    "\n",
    "    hook_fn = functools.partial(direction_ablation_hook,direction=intervention_dir)\n",
    "    act_names = ['resid_pre', 'resid_mid', 'resid_post']\n",
    "    fwd_hooks = [(utils.get_act_name(act_name, l), hook_fn) for l in intervention_layers for act_name in act_names]\n",
    "\n",
    "    # test_examples = random.sample(harmful_inst_test, n_samples)\n",
    "    # test_examples = harmful_inst_test[:eval_samples]\n",
    "    test_examples = test_set[:eval_samples]\n",
    "\n",
    "    intervention_completions = get_generations(\n",
    "        model, test_examples, tokenize_instructions_fn, fwd_hooks=fwd_hooks, batch_size=batch_size)\n",
    "\n",
    "    return intervention_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_store/results_no_pca_top_direction_base.pkl\n",
      "WARNING: Filename ../data_store/results_no_pca_top_direction_base.pkl exists. It will be overwritten\n"
     ]
    }
   ],
   "source": [
    "filename = f'../data_store/results_no_pca_top_direction_{model_type}.pkl'\n",
    "print(filename)\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(f\"WARNING: Filename {filename} exists. It will be overwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples=300, eval_samples=320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:33<00:00, 16.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running base_n_samples_300.layer_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:47<00:00, 20.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = refusal_vector_sample_size\n",
    "eval_samples = len(harmful_test)\n",
    "batch_size = 16\n",
    "\n",
    "print(f\"{n_samples=}, {eval_samples=}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "baseline_completions = get_generations(\n",
    "    model,\n",
    "    harmful_test[:eval_samples],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[],\n",
    "    batch_size=batch_size\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for layer in [14]:\n",
    "    run_id = f\"{model_type}_n_samples_{n_samples}.layer_{layer}\"\n",
    "    print(f\"Running {run_id}\")\n",
    "\n",
    "    refusal_dir, harmful_acts, harmless_acts = get_refusal_direction(\n",
    "        refusal_vector_harmful, refusal_vector_harmless, n_samples=n_samples, layer=layer, batch_size=batch_size)\n",
    "    \n",
    "    intervention_completions = get_intervention_completions(\n",
    "        refusal_dir, eval_samples, harmful_test, batch_size=batch_size)\n",
    "\n",
    "    results.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"n_samples\": n_samples,\n",
    "        \"layer\": layer,\n",
    "        \"refusal_dir\": refusal_dir.cpu().numpy(),\n",
    "        \"harmful_acts\": harmful_acts.cpu().numpy(),\n",
    "        \"harmless_acts\": harmless_acts.cpu().numpy(),\n",
    "        \"baseline_completions\": baseline_completions,\n",
    "        \"intervention_completions\": intervention_completions\n",
    "    })\n",
    "    print()\n",
    "\n",
    "    with open(filename, 'wb') as fw:\n",
    "        pickle.dump(results, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
